0:00:00.000,0:00:01.290

0:00:01.290,0:00:05.660
DAN KLEIN: So the question is, how are
we going to be able to learn how to

0:00:05.660,0:00:11.260
act when we don't know what the actions
do or what rewards we'll get?

0:00:11.260,0:00:14.480
You might think we can just reduce
this to what we had before.

0:00:14.480,0:00:17.840
We can somehow still use value
iteration or expect a max.

0:00:17.840,0:00:19.470
And that's true.

0:00:19.470,0:00:21.900
What we're going to talk about
now is model-based learning.

0:00:21.900,0:00:26.460
In model-based learning, we reduce the
reinforcement learning problem to the

0:00:26.460,0:00:29.010
previous case of a known MDP.

0:00:29.010,0:00:30.210
So how do we do that?

0:00:30.210,0:00:31.740
Well, we try to build a model.

0:00:31.740,0:00:35.670
We try to figure out what the
transitions and rewards do and then

0:00:35.670,0:00:37.820
pretend that's the truth.

0:00:37.820,0:00:39.270
So here's how it works.

0:00:39.270,0:00:42.990
We're going to learn an approximate
model, based on our experiences.

0:00:42.990,0:00:46.830
Once we have that approximate model,
we will solve for the values, for

0:00:46.830,0:00:49.620
example, the optimal values
or policies, as if the

0:00:49.620,0:00:51.260
learn model were correct.

0:00:51.260,0:00:54.680
If we learned a model that's pretty
close, maybe the policy that our

0:00:54.680,0:00:57.960
dynamic programs produce
will be good ones.

0:00:57.960,0:01:01.700
So we'd like to learn an MDP that
we can use for value iteration.

0:01:01.700,0:01:03.220
How are we going to do that?

0:01:03.220,0:01:07.400
Well, we're going to, at times, find
ourselves in certain states, s, and

0:01:07.400,0:01:08.920
take certain actions, a.

0:01:08.920,0:01:12.000
Let's not worry for now about how
we decide what action to take.

0:01:12.000,0:01:16.320
Whenever we're in state s and we take
action a, various things happen.

0:01:16.320,0:01:19.330
So we end up with the various
s prime outcomes.

0:01:19.330,0:01:21.200
Over time, we count them up.

0:01:21.200,0:01:23.670
Some of them happen more and
some of them happen less.

0:01:23.670,0:01:26.580
Maybe one of them happens
90% of the time.

0:01:26.580,0:01:29.760
When we take those counts and normalize
them, meaning divide them by

0:01:29.760,0:01:31.750
their sum, we get probabilities.

0:01:31.750,0:01:35.180
For example, that frequent one
will have probability 0.9.

0:01:35.180,0:01:38.120
That is our estimate of the transition
function for that

0:01:38.120,0:01:39.590
state and action pair.

0:01:39.590,0:01:41.980
We do that for all state
and action pairs.

0:01:41.980,0:01:45.170
In addition, we discover that the
rewards associated with a state s and

0:01:45.170,0:01:49.410
action a and a state s prime whenever
we experience that transition.

0:01:49.410,0:01:51.850
So we're going to act for a while.

0:01:51.850,0:01:54.970
We're going to accumulate counts
of various things.

0:01:54.970,0:01:57.010
We're going to turn them
into probabilities.

0:01:57.010,0:02:00.530
And once that's all done, we have a
transition function t and a reward

0:02:00.530,0:02:01.340
function r.

0:02:01.340,0:02:05.610
They're probably not correct, but they
are structurally what we need to run

0:02:05.610,0:02:07.350
things like value iteration.

0:02:07.350,0:02:09.620
So we learn an MDP, and
then we solve it.

0:02:09.620,0:02:13.820
Now there are a lot of small points here
that are very important, like how

0:02:13.820,0:02:15.410
do you know how to act?

0:02:15.410,0:02:17.880
How do you know how many
counts you need?

0:02:17.880,0:02:19.920
How do you know how close
you're going to be?

0:02:19.920,0:02:22.610
We're not going to get very much
into those details now.

0:02:22.610,0:02:26.700
We'll come back to the idea of how to
learn a good model in the second half

0:02:26.700,0:02:27.950
of the course.

0:02:27.950,0:02:32.170
But for now, I just want to give
you a sketch of how this works.

0:02:32.170,0:02:33.400
Here's an example.

0:02:33.400,0:02:37.300
Let's say we're a grid world and
somebody gives us this policy pi.

0:02:37.300,0:02:40.080
I'm trying to, for now, sidestep the
issue of how we know how to act.l

0:02:40.080,0:02:42.880
we're going to follow this policy
for better or for worse.

0:02:42.880,0:02:44.120
And so what are we going to do?

0:02:44.120,0:02:46.950
We imagine our gamma is one, so there's
no discounting to make this

0:02:46.950,0:02:48.110
example simple.

0:02:48.110,0:02:49.740
And we have some experiences.

0:02:49.740,0:02:53.530
So for example, we might have the
following episodes of training.

0:02:53.530,0:02:57.280
So in these episodes, this is
four times playing the game.

0:02:57.280,0:03:02.290
For example, in the first episode, we
start at B. We go east into C. We go

0:03:02.290,0:03:03.570
east into D.

0:03:03.570,0:03:05.040
And then we take the exit action.

0:03:05.040,0:03:07.620
We transition to the terminal state
where the game is over.

0:03:07.620,0:03:09.930
And we receive our big
reward of plus 10.

0:03:09.930,0:03:12.890
In this particular case, you get a minus
1 living reward and a plus 10

0:03:12.890,0:03:14.040
good exit reward.

0:03:14.040,0:03:15.390
Episode 2 is the same.

0:03:15.390,0:03:21.180
Episode 3 starts in E, goes north into
C, and then east into the good exit.

0:03:21.180,0:03:25.540
Episode 4 is interesting because we
start in E, we go north into C. And

0:03:25.540,0:03:29.530
when we try to go east, we actually
end up in A because there's some

0:03:29.530,0:03:30.550
non-determinism.

0:03:30.550,0:03:32.500
And there, we're forced
to take the bad exit.

0:03:32.500,0:03:34.420
So this is what we've
got to work with.

0:03:34.420,0:03:37.820
In a model-based setting, what we
do with all of this experience--

0:03:37.820,0:03:41.430
which is not everything that could
happen, it's just what we personally

0:03:41.430,0:03:42.510
have seen--

0:03:42.510,0:03:44.890
what we do with this experience is
we try to figure out what the

0:03:44.890,0:03:46.390
transitions must be.

0:03:46.390,0:03:50.560
So we think, all right, I've been in
B. I've taken the action east, and

0:03:50.560,0:03:51.460
what's happened?

0:03:51.460,0:03:55.770
Well so far, 100% of the time, we've
ended up in C. So I'll write down to

0:03:55.770,0:03:59.280
the best of my knowledge right now,
let's just say that east from B has a

0:03:59.280,0:04:03.300
100% chance of resulting in C.
That's a transition function.

0:04:03.300,0:04:09.790
Similarly, east from C, we know that 3/4
of the time it actually went east

0:04:09.790,0:04:13.360
into D. But 1/4 of the time it went
north into A. Are those the right

0:04:13.360,0:04:14.120
probabilities?

0:04:14.120,0:04:15.150
Almost surely not.

0:04:15.150,0:04:18.779
But they're a reasonable starting point
for this simplistic version.

0:04:18.779,0:04:19.910
So what comes out?

0:04:19.910,0:04:22.410
We've learned now a model where
we have transitions.

0:04:22.410,0:04:23.720
This is probability one.

0:04:23.720,0:04:25.750
This is probability 0.75 and so on.

0:04:25.750,0:04:28.080
And we've learned reward functions.

0:04:28.080,0:04:31.110
Now, of course, we've only learned about
the things we've actually tried.

0:04:31.110,0:04:34.250
But let's set that aside until
later in this lecture.

0:04:34.250,0:04:35.980
What do we do with this learned model?

0:04:35.980,0:04:36.950
Well, now it's an MDP.

0:04:36.950,0:04:37.930
It's the wrong MDP.

0:04:37.930,0:04:38.480
Oh well.

0:04:38.480,0:04:40.520
But we still know how to solve it.

0:04:40.520,0:04:43.190
That's the idea of model-based
learning.

0:04:43.190,0:04:46.610
So let's think about exactly
what we just did and what

0:04:46.610,0:04:48.510
model-based learning is.

0:04:48.510,0:04:51.000
Let's imagine we had a much,
much simpler problem.

0:04:51.000,0:04:54.620
Let's imagine that my goal was to
compute the average age of people in

0:04:54.620,0:04:55.920
this class.

0:04:55.920,0:04:58.370
So I want to compute the expected
age, the weighted average.

0:04:58.370,0:04:59.600
How would I do that?

0:04:59.600,0:05:01.990
Well, we all know how to
compute expectations.

0:05:01.990,0:05:06.110
And so, for example, if I knew the
probability distribution over ages,

0:05:06.110,0:05:10.220
how many people are each age, then I
would have a straightforward way of

0:05:10.220,0:05:12.160
computing this weighted average.

0:05:12.160,0:05:15.680
And the way I do that is I'd say, well,
the expectation of this random

0:05:15.680,0:05:19.530
variable, A, I sum over all
of the possible ages.

0:05:19.530,0:05:22.560
And I weight each one
by its probability.

0:05:22.560,0:05:24.450
So it's just a weighted average
according to the

0:05:24.450,0:05:26.040
distribution over ages.

0:05:26.040,0:05:26.940
Easy.

0:05:26.940,0:05:30.610
In fact, this is all that's really
going on in these MDPs.

0:05:30.610,0:05:32.030
This is what happens at a chance node.

0:05:32.030,0:05:34.490
You take a weighted expectation
of values that are

0:05:34.490,0:05:35.940
one step in the future.

0:05:35.940,0:05:36.600
So here it is.

0:05:36.600,0:05:37.870
Here's the weighted expectation.

0:05:37.870,0:05:38.870
What's that mean?

0:05:38.870,0:05:43.560
That means I actually look up for each
value, oh, how many people are age 20?

0:05:43.560,0:05:44.920
Oh, that's 0.35.

0:05:44.920,0:05:46.270
And then I continue.

0:05:46.270,0:05:48.070
That's like solving an MDP.

0:05:48.070,0:05:49.930
You know the probability distribution.

0:05:49.930,0:05:54.770
And so you just compute the expectation
by summing everything.

0:05:54.770,0:05:57.630
Now what happens if we don't
know the distribution?

0:05:57.630,0:05:59.620
We're forced to rely on samples.

0:05:59.620,0:06:00.610
We could take a few samples.

0:06:00.610,0:06:01.900
We could take a lot of samples.

0:06:01.900,0:06:04.590
Obviously, the more samples, all
else equal, the more accurate

0:06:04.590,0:06:05.890
these things will be.

0:06:05.890,0:06:08.280
We're going to collect samples,
instead of the probability

0:06:08.280,0:06:09.500
distribution.

0:06:09.500,0:06:14.210
The model-based view is, all right, I
don't know the distribution over ages,

0:06:14.210,0:06:15.450
but I've got samples.

0:06:15.450,0:06:17.950
What do I do with the samples?

0:06:17.950,0:06:22.270
The model-based view is, I use
the samples to reconstruct an

0:06:22.270,0:06:24.450
approximation of what I didn't know.

0:06:24.450,0:06:28.070
So I say, all right, my empirical
distribution over ages is whatever the

0:06:28.070,0:06:29.380
samples say.

0:06:29.380,0:06:32.950
And now I've reduced it to the case
of the known distribution.

0:06:32.950,0:06:36.440
And I compute expectations the way I
always do, by summing over all of the

0:06:36.440,0:06:40.390
different ages and weighting each one
by their relative frequencies in the

0:06:40.390,0:06:41.390
distribution.

0:06:41.390,0:06:43.330
Why does this model-based thing work?

0:06:43.330,0:06:47.830
The reason it works is because,
eventually, you learn the right model.

0:06:47.830,0:06:48.940
So what else could we do?

0:06:48.940,0:06:52.650
We could stay in the same setting where
we don't know P of A. And we

0:06:52.650,0:06:55.620
could do what's called model-free
approaches here.

0:06:55.620,0:06:59.950
What we do in a model-free approach is
we average the samples directly.

0:06:59.950,0:07:02.220
So for example, this person's 20.

0:07:02.220,0:07:03.410
This person's 30.

0:07:03.410,0:07:06.270
I add up all of the samples and
give them equal weight.

0:07:06.270,0:07:09.050
And I take their, in this case,
unweighted average.

0:07:09.050,0:07:10.020
It's super important.

0:07:10.020,0:07:13.030
When I do this model-free stuff,
the averages are unweighted.

0:07:13.030,0:07:14.720
Why does that work?

0:07:14.720,0:07:19.450
That works because the ages that are
more frequent show up in more samples.

0:07:19.450,0:07:22.650
So they're already up-weighted by virtue
of the fact that they pop up

0:07:22.650,0:07:25.040
more often in the sample outcomes.

0:07:25.040,0:07:27.950
And because the samples are going to
appear with the right frequencies,

0:07:27.950,0:07:30.780
either approach is going
to do the same thing.

0:07:30.780,0:07:33.100
Now in this case, you look at this
and you think, it's pretty

0:07:33.100,0:07:34.170
much exactly the same.

0:07:34.170,0:07:36.970
It's just about how and
when you group things.

0:07:36.970,0:07:38.980
Nothing important is happening here.

0:07:38.980,0:07:42.500
And that's because there's no structure
to this example problem.

0:07:42.500,0:07:44.870
The algorithms are going to be very
different in the reinforcement

0:07:44.870,0:07:47.330
learning case, because there's
going to be structure.

0:07:47.330,0:07:50.250
Things are going to depend on the
other states around them.

0:07:50.250,0:07:53.500
But this is the high-level view of
model-based versus model-free.

0:07:53.500,0:07:57.050
In model-based, you learn the
probability distributions.

0:07:57.050,0:08:01.440
And then you reduce it to the
case of solving a known MDP.

0:08:01.440,0:08:04.930
In the model-free case, we just take our
samples as they come and average

0:08:04.930,0:08:06.340
them together.

0:08:06.340,0:08:09.940
And while the model-based way may seem
more comfortable at first, because it

0:08:09.940,0:08:13.230
reduces to a known case, we'll see that
the algorithms for the model-free

0:08:13.230,0:08:15.360
case are, in many cases, much simpler.

0:08:15.360,0:08:16.610

