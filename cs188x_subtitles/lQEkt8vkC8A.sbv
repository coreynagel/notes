0:00:00.000,0:00:01.290

0:00:01.290,0:00:01.530
DAN KLEIN: All right.

0:00:01.530,0:00:05.180
Now let's talk about an important
algorithm called policy iteration,

0:00:05.180,0:00:10.990
which combines the idea of evaluating
one policy with the idea of improving

0:00:10.990,0:00:14.070
that policy on the basis
of those values.

0:00:14.070,0:00:18.140
You can think of policy iteration as
you've got a policy in front of you,

0:00:18.140,0:00:21.980
and you're constantly trying
to make it better.

0:00:21.980,0:00:26.220
So let's think about y value iteration
as not always the best solution.

0:00:26.220,0:00:29.380
Well, what value iteration
does is essentially

0:00:29.380,0:00:31.430
mimics the Bellman updates.

0:00:31.430,0:00:35.760
You have iterations where k gets larger
and larger, starting at zero.

0:00:35.760,0:00:39.250
And for each iteration,
you visit each state.

0:00:39.250,0:00:42.600
And for each state, you
look at each action.

0:00:42.600,0:00:46.990
And for each action, you
look at each outcome.

0:00:46.990,0:00:49.890
One problem with that process
is it's slow.

0:00:49.890,0:00:54.040
And the reason it's slow is each
iteration not only looks at every

0:00:54.040,0:00:59.450
source and target state, and so is s
squared, it also has to consider each

0:00:59.450,0:01:01.270
action each time.

0:01:01.270,0:01:04.150
And there are often many actions.

0:01:04.150,0:01:07.870
Another problem is that although
you're considering all of these

0:01:07.870,0:01:11.590
actions, the maximum often
doesn't change.

0:01:11.590,0:01:13.810
Let's see a demo of that.

0:01:13.810,0:01:15.690
So here is a grid.

0:01:15.690,0:01:18.030
This is sum setting of grid world.

0:01:18.030,0:01:19.740
And let's watch what happens.

0:01:19.740,0:01:22.500
At v0, all the values are going
to be zero because there is

0:01:22.500,0:01:24.770
0 time steps left.

0:01:24.770,0:01:27.040
At v1, you can see one
step into the future.

0:01:27.040,0:01:29.420
If you're at an exit, you
get the exit value.

0:01:29.420,0:01:32.410
If you're anywhere else, you get just
the living reward, which in this case

0:01:32.410,0:01:33.990
is minus 0.1.

0:01:33.990,0:01:38.940
If I look further into the future, we
know that in value iteration, the

0:01:38.940,0:01:41.690
rewards from that plus 1 are
going to propagate out.

0:01:41.690,0:01:44.300

0:01:44.300,0:01:46.230
This is nine iterations in.

0:01:46.230,0:01:49.880
If I run it to convergence,
things keep changing.

0:01:49.880,0:01:50.960
Why is that?

0:01:50.960,0:01:55.290
That's because there are very long
sequences that still give you points

0:01:55.290,0:02:00.520
from that plus 1, that take a while
to register in the lower left.

0:02:00.520,0:02:02.500
But let's look a little more closely.

0:02:02.500,0:02:07.710
What I want you to watch this time is I
want you to watch as I increase k in

0:02:07.710,0:02:08.860
value iteration.

0:02:08.860,0:02:10.870
I want you to watch the arrows.

0:02:10.870,0:02:12.410
And we're going to this twice.

0:02:12.410,0:02:15.730
First I want you to watch
them changing.

0:02:15.730,0:02:17.340
So at the beginning, they're
going to change a lot.

0:02:17.340,0:02:23.030

0:02:23.030,0:02:26.980
But eventually, they stop changing.

0:02:26.980,0:02:30.050
And then even when I run into 100,
they still haven't changed since

0:02:30.050,0:02:31.670
iteration eight or nine.

0:02:31.670,0:02:35.830
Secondly, watch in kind
of the lower left.

0:02:35.830,0:02:42.310
These numbers are changing, but it takes
a while before the change is to

0:02:42.310,0:02:46.430
become large enough that they
impact one of the arrows.

0:02:46.430,0:02:48.500
So now we're in a position
where we can see these

0:02:48.500,0:02:50.370
problems with value iteration.

0:02:50.370,0:02:53.840
Each iteration is slow because it
considers all of the actions.

0:02:53.840,0:02:58.500
But that maximum, that best action that
you got when you considered them

0:02:58.500,0:03:01.670
all usually doesn't change.

0:03:01.670,0:03:06.260
So you wasted all that time, if you have
100 actions, checking the other

0:03:06.260,0:03:10.250
99 that weren't very good last round.

0:03:10.250,0:03:14.140
They're still not very good, but
you still check them all.

0:03:14.140,0:03:18.900
In addition, as we saw in that example,
the policy may be converged

0:03:18.900,0:03:21.600
when the values are nowhere
near converged yet.

0:03:21.600,0:03:27.480
So the policy tends to finish long
before the values converge.

0:03:27.480,0:03:29.870
So what can we do?

0:03:29.870,0:03:33.320
The idea here is an algorithm called
policy iteration, which is an

0:03:33.320,0:03:34.850
alternative approach.

0:03:34.850,0:03:39.230
And the basic sketch is we're going to
have two steps that we alternate.

0:03:39.230,0:03:41.800
Step one is policy evaluation.

0:03:41.800,0:03:43.930
We're going to have some fixed policy.

0:03:43.930,0:03:47.120
It's generally not going to be an
optimal policy, but we're going to

0:03:47.120,0:03:50.340
figure out its values using
policy evaluation.

0:03:50.340,0:03:54.710
Which if we remember, was fast because
it didn't have that factor of a.

0:03:54.710,0:03:57.000
So we evaluate the policy we
have, even though it's

0:03:57.000,0:03:59.290
not an optimal policy.

0:03:59.290,0:04:05.790
Then we take those values, and we
extract a better policy from them.

0:04:05.790,0:04:07.870
That's called policy improvement.

0:04:07.870,0:04:13.190
And the way we do policy improvement is
we extract the policy implied by a

0:04:13.190,0:04:18.279
one step look ahead from those values
we just computed in step one.

0:04:18.279,0:04:21.730
We repeat these until the policy
converges, and it turns out you're

0:04:21.730,0:04:24.100
guaranteed that the policy
will always keep getting

0:04:24.100,0:04:26.740
better, and then converge.

0:04:26.740,0:04:28.590
This algorithm is policy iteration.

0:04:28.590,0:04:32.590
It's still optimal, and it can converge
much faster under some

0:04:32.590,0:04:33.740
conditions.

0:04:33.740,0:04:37.180
The conditions under which it converges
much faster tend to be ones

0:04:37.180,0:04:40.370
where there are a large number of
actions, but where the maximizing

0:04:40.370,0:04:45.790
action doesn't change very much
during value iteration rounds.

0:04:45.790,0:04:49.320
If we wrote this out in equations, it
would look something like this.

0:04:49.320,0:04:53.500
Step one is evaluation, so I fix
some policy, pi, and I'm going

0:04:53.500,0:04:55.670
to compute v pi.

0:04:55.670,0:04:58.760
That is we asked before why would
we want to compute the

0:04:58.760,0:05:00.310
values for a fixed policy?

0:05:00.310,0:05:01.330
Here's a case.

0:05:01.330,0:05:03.570
I fixed pi, and I compute the values.

0:05:03.570,0:05:08.470
And so I do this modified, or simplified
update where I don't max

0:05:08.470,0:05:09.840
over the actions.

0:05:09.840,0:05:14.970
I assume that my policy pi is fixed,
and I only do the averaging at the

0:05:14.970,0:05:15.830
chance nodes.

0:05:15.830,0:05:19.930
So this thing is fast, because I
don't consider all the actions.

0:05:19.930,0:05:25.210
Once this thing converges, or gets close
enough, I then stop and I say,

0:05:25.210,0:05:29.230
let's give the actions a chance to
change, because we've done enough of

0:05:29.230,0:05:34.540
this evaluation that maybe by now, some
of the actions need to change.

0:05:34.540,0:05:37.980
So again, we're going to do a one step
look ahead, and just unroll that

0:05:37.980,0:05:43.030
expectimax one layer, and say the new
policy is the action that I would get

0:05:43.030,0:05:45.590
if I did a one step expectimax.

0:05:45.590,0:05:50.440
And then plugged in the values, not
optimal values, but the values

0:05:50.440,0:05:53.430
computed against my old policy.

0:05:53.430,0:05:54.790
That's a little weird, right?

0:05:54.790,0:05:58.340
We do one step of actual expectimax,
and our truncation

0:05:58.340,0:06:00.990
function is our old policy.

0:06:00.990,0:06:02.180
Now, why should that be better?

0:06:02.180,0:06:06.670
Well, maybe the policy was good, or
maybe the old policy was bad.

0:06:06.670,0:06:09.510
But whatever it was, good or bad,
we've just pushed it one

0:06:09.510,0:06:11.530
step into the future.

0:06:11.530,0:06:16.260
And that step where we did that one
layer of expectimax, that layer,

0:06:16.260,0:06:20.600
insofar as a layer can be optimal,
did the optimal computation.

0:06:20.600,0:06:24.780
So we've got one layer of real optimal
expectimax, and then the truncation

0:06:24.780,0:06:27.140
function is the old values.

0:06:27.140,0:06:30.740
And every time we do this, we push the
old stuff farther and farther into the

0:06:30.740,0:06:34.100
future, and thereby make progress.

0:06:34.100,0:06:38.550
If you think about this, it actually
turns out the improvement step is just

0:06:38.550,0:06:39.850
like value iteration.

0:06:39.850,0:06:43.390
We loop over all the actions, and for
each action, we loop over all of the

0:06:43.390,0:06:45.750
results s prime.

0:06:45.750,0:06:49.040
So the improvement step isn't going
to be particularly fast.

0:06:49.040,0:06:51.745
It's essentially the evaluation
step where we get a big speed

0:06:51.745,0:06:53.550
up over value iteration.

0:06:53.550,0:06:56.790
And because the evaluation step happens
many times before we allow an

0:06:56.790,0:07:00.130
improvement step, we
get a big speed up.

0:07:00.130,0:07:02.960
Another way of looking at this algorithm
is thinking that we're doing

0:07:02.960,0:07:07.880
value iteration, but on most rounds we
just go with the last action that

0:07:07.880,0:07:11.340
optimized for the state, rather
than considering them all.

0:07:11.340,0:07:13.110
OK, let's compare them.

0:07:13.110,0:07:16.530
Both value iteration and policy
iteration do the same thing.

0:07:16.530,0:07:21.460
You input an MDP, and what is output
is an optimal quantity.

0:07:21.460,0:07:25.350
And on the surface that's the optimal
values, but they both also provide

0:07:25.350,0:07:26.640
optimal policies.

0:07:26.640,0:07:30.390
In the case of policy iteration,
it explicitly provides that.

0:07:30.390,0:07:33.550
In the case of value iteration, you
just keep track of it as you go.

0:07:33.550,0:07:35.900
So they both compute optimal values.

0:07:35.900,0:07:41.280
In value iteration, every iteration
updates the values and implicitly by

0:07:41.280,0:07:44.396
doing this max every time, the policy.

0:07:44.396,0:07:48.930
In policy iteration, we do lots of
passes that only update the utilities,

0:07:48.930,0:07:51.670
the values for this fixed policy.

0:07:51.670,0:07:55.270
Only when we do a policy improvement
step do we allow the policy to change,

0:07:55.270,0:07:58.030
and because of that,
it's often faster.

0:07:58.030,0:08:00.390
Both of them of the same
class of thing.

0:08:00.390,0:08:04.610
They are both dynamic programs which
take an MDP as input, and produce

0:08:04.610,0:08:07.660
optimal values and policies
as outputs.

0:08:07.660,0:08:09.580
We've got a lot of algorithms by now.

0:08:09.580,0:08:13.780
Even setting aside expectimax, which
was a way of choosing actions in an

0:08:13.780,0:08:16.650
MDP, we've still got a lot
of different things.

0:08:16.650,0:08:20.110
If you want to compute optimal values,
we've got value iteration, and we've

0:08:20.110,0:08:22.480
got policy iteration, which
do the same thing.

0:08:22.480,0:08:26.050
Policy iteration's a little more
complex, but it's often faster.

0:08:26.050,0:08:30.380
If all you wanted to do was compute
values for a particular policy, we

0:08:30.380,0:08:32.210
have policy evaluation.

0:08:32.210,0:08:36.299
If all you wanted to do was turn your
values into a policy, we have policy

0:08:36.299,0:08:40.169
extraction, which you can think of
as just a one step look ahead.

0:08:40.169,0:08:44.390
Depending on what you have as input, a
policy or not, and what you want as

0:08:44.390,0:08:48.270
output, optimal quantities or just the
evaluation of a single policy, that

0:08:48.270,0:08:51.800
determines what equations you use
and what algorithm you use.

0:08:51.800,0:08:54.680
And so you might be saying,
these all look the same.

0:08:54.680,0:08:57.050
That is a good reaction to have.

0:08:57.050,0:08:58.650
They basically are all the same.

0:08:58.650,0:09:02.740
These are all Bellman updates turned
into an iterative algorithm.

0:09:02.740,0:09:06.080
They all use a one step look
ahead expectimax fragment.

0:09:06.080,0:09:09.720
The difference is whether or not you
plug in a fixed policy, or actually do

0:09:09.720,0:09:11.860
a max over the actions.

0:09:11.860,0:09:13.660
That's it.

0:09:13.660,0:09:16.910
So once you really understand these
algorithms, you'll start feeling like,

0:09:16.910,0:09:20.760
oh, it's all just Bellman equations and
the corresponding dynamic program.

0:09:20.760,0:09:22.010
And that's a great place to be.

0:09:22.010,0:09:23.150

