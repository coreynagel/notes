0:00:00.000,0:00:01.365

0:00:01.365,0:00:02.120
PROFESSOR: All right.

0:00:02.120,0:00:05.400
Now that we know what Markov decision
processes are, we can talk about how

0:00:05.400,0:00:07.570
to solve them.

0:00:07.570,0:00:11.080
For solving MDPs, there's a bunch of
important quantities we need to define

0:00:11.080,0:00:14.850
precisely so that we can come up with
algorithms that compute them.

0:00:14.850,0:00:17.200
Some of these quantities you already
know, or you know something

0:00:17.200,0:00:18.820
very much like them.

0:00:18.820,0:00:23.210
So the first one is the notion of the
utility or the value of a state s.

0:00:23.210,0:00:24.850
So you actually already know this.

0:00:24.850,0:00:28.870
This is just what would happen from
that state if you ran Expectimax.

0:00:28.870,0:00:34.230
And in particular, to define
it, we can write V* of s.

0:00:34.230,0:00:38.040
And what the star means is this is
the value under optimal action.

0:00:38.040,0:00:44.120
V* of s is the expected utility starting
in s and acting optimally.

0:00:44.120,0:00:47.630
The idea here is, if you act optimally
from s, sometimes you'll get big

0:00:47.630,0:00:50.150
rewards and sometimes you'll get small
rewards, because you don't actually

0:00:50.150,0:00:51.650
control your outcomes.

0:00:51.650,0:00:54.710
But on average, V* s is
the best you can do.

0:00:54.710,0:00:57.830
That is actually what
Expectimax computed.

0:00:57.830,0:01:01.310
So states have values which are
their Expectimax utilities.

0:01:01.310,0:01:06.590
Remember a q-state, s, a, is being in
the situation where you are in state s

0:01:06.590,0:01:10.660
and you are committed to action a,
but you haven't done it yet.

0:01:10.660,0:01:13.360
So anything could still happen.

0:01:13.360,0:01:18.940
The value or utility of a q-state
is written Q* of s comma a.

0:01:18.940,0:01:23.800
That is the expected utility from
that q-state if you act

0:01:23.800,0:01:26.010
optimally in the future.

0:01:26.010,0:01:29.150
Now if we think about this in
the tree, the values are

0:01:29.150,0:01:31.000
associated with states.

0:01:31.000,0:01:35.270
So we can write the value of a state and
say, that's what would happen if

0:01:35.270,0:01:39.090
you ran Expectimax from a state, meaning
you started out by picking the

0:01:39.090,0:01:41.670
best action and then recursed.

0:01:41.670,0:01:45.980
If, however, you run Expectimax starting
at a chance node, which here

0:01:45.980,0:01:52.120
are q-states, you would still get some
optimal thing, but it would be the

0:01:52.120,0:01:54.580
best you can do starting
at that chance node.

0:01:54.580,0:01:57.520
What does it mean to start at
a q-state or a chance node?

0:01:57.520,0:01:59.440
It means you are already
taking that action.

0:01:59.440,0:02:01.860
It's too late to decide a
wasn't the right thing.

0:02:01.860,0:02:03.750
Whatever is going to happen
is going to happen.

0:02:03.750,0:02:09.000
And then from s-prime forward,
you will behave optimally.

0:02:09.000,0:02:12.220
The third important quantity is pi*.

0:02:12.220,0:02:13.590
Pi is a policy.

0:02:13.590,0:02:15.510
Pi* is an optimal policy.

0:02:15.510,0:02:19.770
And pi* from s is the optimal
action from state s.

0:02:19.770,0:02:22.960
Let's take a look at what these
values actually are.

0:02:22.960,0:02:26.300
So here we see that same grid.

0:02:26.300,0:02:31.030
And now what's also shown are numbers
in the squares and arrows.

0:02:31.030,0:02:32.460
The arrows are easy.

0:02:32.460,0:02:34.950
The arrows are the policy
that has been computed.

0:02:34.950,0:02:37.500
And in this case, it's
the optimal policy.

0:02:37.500,0:02:39.950
Now in order to know whether we actually
got it right, I need to tell

0:02:39.950,0:02:42.300
you which setting of
grid world this is.

0:02:42.300,0:02:48.220
And in this case, this is a setting
where the living reward is minus 0.1.

0:02:48.220,0:02:53.800
And there's a small decay
or discount of 0.99.

0:02:53.800,0:02:57.480
So we can see the values of these
states, because the only thing that

0:02:57.480,0:03:01.070
can happen in these states is that you
choose exit, and then you get the

0:03:01.070,0:03:02.780
reward deterministically.

0:03:02.780,0:03:06.420
These states have values
of 1 and minus 1.

0:03:06.420,0:03:11.270
If you're down here in the lower left,
the policy suggests that you, over

0:03:11.270,0:03:17.530
time, make your way up and then over,
and then take the good exit.

0:03:17.530,0:03:19.290
This 0.49--

0:03:19.290,0:03:20.930
what's that?

0:03:20.930,0:03:25.760
That is, if you start in this state and
you ran this game over and over

0:03:25.760,0:03:29.150
again, and sometimes you slipped and
sometimes you didn't, and you added up

0:03:29.150,0:03:34.170
all of those utilities, on average
you would get 0.49.

0:03:34.170,0:03:37.410
And you would achieve it by trying
to go north whenever you

0:03:37.410,0:03:39.090
were in this state.

0:03:39.090,0:03:41.970
So you can see it's better to be
right here by the exit than it

0:03:41.970,0:03:43.470
is to be over here.

0:03:43.470,0:03:43.900
Why?

0:03:43.900,0:03:47.430
Because if you're over here, you have
to pay that living reward and some

0:03:47.430,0:03:49.580
discount to even get to the exit.

0:03:49.580,0:03:52.500
The best thing is actually
be in the exit.

0:03:52.500,0:03:56.140
Similarly, you can see, if you're
essentially two steps--

0:03:56.140,0:03:57.730
assuming nothing goes wrong--

0:03:57.730,0:04:03.050
from the exit, it's better to be up here
at 0.74 than down here at 0.57.

0:04:03.050,0:04:06.280
Because when something goes wrong
at 0.74, it's not a big deal.

0:04:06.280,0:04:09.940
When something goes wrong at 0.57,
you fall into the pit and lose.

0:04:09.940,0:04:12.640
So that's what the utilities
look like, and also the

0:04:12.640,0:04:16.720
policies on this grid.

0:04:16.720,0:04:18.339
So these are utilities.

0:04:18.339,0:04:20.410
These are the values of the states.

0:04:20.410,0:04:23.700
Let's in particular remember 0.57.

0:04:23.700,0:04:28.410
0.57 is the value of being in the state,
assuming you are smart in the

0:04:28.410,0:04:31.610
future, which means in particular
from the state going north.

0:04:31.610,0:04:34.510

0:04:34.510,0:04:35.900
What's this?

0:04:35.900,0:04:37.970
This is the q-values.

0:04:37.970,0:04:41.280
From each state, except for the
exit states, you've got

0:04:41.280,0:04:43.550
four choices of action.

0:04:43.550,0:04:47.860
If you are in this state where you're
between the wall and the pit, and the

0:04:47.860,0:04:52.710
action you've committed to is north,
then you get that same 0.57.

0:04:52.710,0:04:54.300
That's where the value comes from.

0:04:54.300,0:04:57.580
It comes from going north and
then acting optimally.

0:04:57.580,0:05:02.770
If, however, you're in the q-state of
being in this square and committing to

0:05:02.770,0:05:07.070
going right, your q-value is a lot
lower, because probably you're going

0:05:07.070,0:05:08.020
to fall into the pit.

0:05:08.020,0:05:08.970
Now, you might not.

0:05:08.970,0:05:11.530
And because of the definition of a
q-value, if you don't, you will then

0:05:11.530,0:05:13.870
act optimally in the future,
which will involve not

0:05:13.870,0:05:15.540
trying for the pit again.

0:05:15.540,0:05:21.550
But the point is each one of these
states has four q-values, but only one

0:05:21.550,0:05:23.600
value-- and unsurprisingly,
the value is going to be

0:05:23.600,0:05:26.530
the max of the q-values.

0:05:26.530,0:05:28.440
So we want to be able to
compute these things.

0:05:28.440,0:05:33.480
We'd like to be able to take an MDP and
compute these Expectimax values

0:05:33.480,0:05:34.620
for a state.

0:05:34.620,0:05:37.670
And actually, what we usually do with
these algorithms is we compute the

0:05:37.670,0:05:39.470
values for all of the states.

0:05:39.470,0:05:42.530
And we'll see that there are ways to
save time by doing all the states at

0:05:42.530,0:05:45.560
once, provided your MDP is small
enough that you can actually go

0:05:45.560,0:05:47.710
through all the states.

0:05:47.710,0:05:48.870
What are we computing?

0:05:48.870,0:05:51.660
We're computing the expecting utility
under optimal actions.

0:05:51.660,0:05:54.250
And that's going to be a sum
of discounted rewards.

0:05:54.250,0:05:57.770
Because this is what Expectimax
computed, we can write down a

0:05:57.770,0:06:01.620
mathematical definition of this thing
we're trying to compute that basically

0:06:01.620,0:06:05.410
parallels Expectimax, written
out in equations.

0:06:05.410,0:06:09.010
So where we used to have a recursive
function, now we're just going to have

0:06:09.010,0:06:11.110
equations that mention each other.

0:06:11.110,0:06:12.220
So let's take a look.

0:06:12.220,0:06:16.320
Well, from a state, we would
like to know, what is the

0:06:16.320,0:06:18.410
optimal value of a state?

0:06:18.410,0:06:20.775
That's Expectimax from
the root of the tree.

0:06:20.775,0:06:23.300
Well, what is it?

0:06:23.300,0:06:26.810
I'm not really sure, but I've got all
these actions I can choose from.

0:06:26.810,0:06:29.080
And I should maximize over them.

0:06:29.080,0:06:33.310
And I can write that as an equation,
as a max over the actions

0:06:33.310,0:06:35.420
available to me.

0:06:35.420,0:06:41.810
And for each action, if I go down in
the tree, I end up at a q-state.

0:06:41.810,0:06:43.600
And so I can essentially recurse.

0:06:43.600,0:06:44.660
What does that mean in an equation?

0:06:44.660,0:06:51.250
Well, I just write down the quantity
that's below, which is the value of a

0:06:51.250,0:06:57.030
state is the max over all the actions
available to you of the q-values, of

0:06:57.030,0:07:00.670
the chance nodes you would get if
you committed to that action.

0:07:00.670,0:07:03.830
That's just what the max function
of Expectimax did.

0:07:03.830,0:07:06.870
It's a max over all the chance nodes.

0:07:06.870,0:07:12.110
The tricky one, and the one that's
actually changed, is what do I do at a

0:07:12.110,0:07:13.360
chance node?

0:07:13.360,0:07:19.380
So in this case, I'm
here at s comma a.

0:07:19.380,0:07:23.225
And I want to know how many points I'm
going to get under optimal action.

0:07:23.225,0:07:25.390
It kind of depends what happens.

0:07:25.390,0:07:27.240
But I know what's going to happen.

0:07:27.240,0:07:31.270
Whatever state, s-prime, I land in,
I'm going to get some reward.

0:07:31.270,0:07:35.110
The reward is going to depend on s and
a, but also on whether or not the

0:07:35.110,0:07:36.030
outcome was good.

0:07:36.030,0:07:39.680
So the reward depends on
the whole transition.

0:07:39.680,0:07:43.200
That's going to be my one-time
step of reward.

0:07:43.200,0:07:45.020
But of course I'm going to
keep getting rewards from

0:07:45.020,0:07:47.420
s-prime when I land.

0:07:47.420,0:07:51.420
So I'm also going to get all of the
future rewards from s-prime.

0:07:51.420,0:07:54.590
But that's just a value.

0:07:54.590,0:07:57.620
So again, you see the
mutual recursion.

0:07:57.620,0:07:59.890
Now there's something missing here.

0:07:59.890,0:08:02.350
There's actually a couple
things missing here.

0:08:02.350,0:08:07.410
One thing that's missing is I don't know
which s-prime is going to happen.

0:08:07.410,0:08:10.000
And so at a chance node, what do I do?

0:08:10.000,0:08:13.850
What I do with all of these different
possible outcomes, s-prime?

0:08:13.850,0:08:17.640
I have to consider them all and I
have to take a weighted average.

0:08:17.640,0:08:21.647
And the way I write that in math is I
write that I'm going to sum up over

0:08:21.647,0:08:24.650
all s-primes.

0:08:24.650,0:08:27.340
And they each get a weight.

0:08:27.340,0:08:30.670
And that weight is given by
the transition function.

0:08:30.670,0:08:35.700
And it is the probability of s-prime,
given s and a, which we write as T of

0:08:35.700,0:08:37.520
s, a, s-prime.

0:08:37.520,0:08:39.959
So let's look at the pieces and
make sure we understand.

0:08:39.959,0:08:42.760
We're going to get a reward
in the next timestep, and

0:08:42.760,0:08:44.870
then a future reward.

0:08:44.870,0:08:48.950
They're going to be weighted by the
relative probabilities that come from

0:08:48.950,0:08:50.260
our transition function.

0:08:50.260,0:08:53.640
And we're missing one small
but important thing.

0:08:53.640,0:08:59.650
And that is V* is the score
we would get from s-prime.

0:08:59.650,0:09:02.850
But it's happening one timestep
in the future.

0:09:02.850,0:09:04.410
So it's worth less to me.

0:09:04.410,0:09:06.570
How much less?

0:09:06.570,0:09:08.170
Gamma less.

0:09:08.170,0:09:12.440
And now we have the recursive
expression for Q*.

0:09:12.440,0:09:14.200
And these are the pieces.

0:09:14.200,0:09:16.100
You have a reward and a future value.

0:09:16.100,0:09:19.030
You have a discount on
the future value.

0:09:19.030,0:09:21.030
And you have probabilities
that average together

0:09:21.030,0:09:22.370
the different outcomes.

0:09:22.370,0:09:25.140
So this expression may look complicated,
but you're going to be

0:09:25.140,0:09:28.810
seeing this so many times in so many
variations that it's important to make

0:09:28.810,0:09:33.060
sure you don't go into math shock
and you actually process it.

0:09:33.060,0:09:38.570
To that end, I am going to make
it appear in beautiful LaTeX.

0:09:38.570,0:09:43.350
Even though your code alternates between
maximizing in one function and

0:09:43.350,0:09:47.500
taking an average in another function,
it's often the case that

0:09:47.500,0:09:51.160
mathematically we essentially
inline q.

0:09:51.160,0:09:55.280
And we write the values V*
in terms of other values.

0:09:55.280,0:09:56.420
Let's try to do that.

0:09:56.420,0:09:58.500
Let's try to do it in one step.

0:09:58.500,0:10:01.440
I'm basically going to write the exact
same Expectimax recurrence.

0:10:01.440,0:10:09.110
I'm going to say, well, from here I
would get a reward on my transition,

0:10:09.110,0:10:13.910
and then a discounted future value
from wherever I land in s-prime.

0:10:13.910,0:10:17.090

0:10:17.090,0:10:21.080
And this would all depend
on which action I chose.

0:10:21.080,0:10:23.070
And I should choose the best one.

0:10:23.070,0:10:27.070
And it would depend on the
outcome of that action.

0:10:27.070,0:10:31.090
And so I need to take a weighted
average according to T.

0:10:31.090,0:10:34.510
So this is an expression of
values in terms of values.

0:10:34.510,0:10:37.380
As a function, it would
be mutually recursive.

0:10:37.380,0:10:39.960
As an equation, it's a
system of equations.

0:10:39.960,0:10:42.450
And because there's a max in
there, it's non-linear.

0:10:42.450,0:10:45.380
We'll come back to this
equation many times.

0:10:45.380,0:10:47.210
It's in fact what's called
a Bellman equation.

0:10:47.210,0:10:49.610
We'll be seeing this a lot
starting next lecture.

0:10:49.610,0:10:53.330

0:10:53.330,0:10:55.440
So here's that racing search tree.

0:10:55.440,0:10:57.730
And remember, we looked at this and we
said, all right, we just wrote some

0:10:57.730,0:11:00.260
equations, but they basically said
to run Expectimax, right?

0:11:00.260,0:11:02.250
You want the thing at the
root, so you take a max.

0:11:02.250,0:11:04.140
You want a chance node, so
you take an average.

0:11:04.140,0:11:06.420
Yeah, there was some discounting thrown
in, but I can do that with

0:11:06.420,0:11:07.540
Expectimax.

0:11:07.540,0:11:08.430
OK, fine.

0:11:08.430,0:11:10.800
But the racing tree is shown
here for two steps.

0:11:10.800,0:11:12.180
What happens if we go further?

0:11:12.180,0:11:13.740
Well, here, it looks like this.

0:11:13.740,0:11:15.700
It gets very big very quickly.

0:11:15.700,0:11:19.390
But if I look at this, I might think
that Expectimax is somehow a colossal

0:11:19.390,0:11:22.590
waste of time for this problem.

0:11:22.590,0:11:27.280
Why is Expectimax such a
colossal waste of time?

0:11:27.280,0:11:29.940
Basically, we're doing way too
much work with Expectimax.

0:11:29.940,0:11:31.240
And there's two reasons.

0:11:31.240,0:11:34.700
One reason why Expectimax is
wasting a lot of time is

0:11:34.700,0:11:36.070
that states are repeated.

0:11:36.070,0:11:40.220
So you look at this, you see that blue
car that's in the cool state over and

0:11:40.220,0:11:43.060
over and over, and the red car
over and over and over.

0:11:43.060,0:11:46.300
And somehow, we shouldn't be computing
that value over and over and over,

0:11:46.300,0:11:48.820
because it's always going
to be the same.

0:11:48.820,0:11:51.350
And this should remind you, basically,
of graph search.

0:11:51.350,0:11:52.600
We don't want to do the work again.

0:11:52.600,0:11:55.200

0:11:55.200,0:11:59.400
In addition to having lots of repeated
states, the tree also has the problem

0:11:59.400,0:12:03.010
that it's infinite, and it's hard
to compute infinite things.

0:12:03.010,0:12:04.770
So this tree goes on forever.

0:12:04.770,0:12:08.400
And we already have an idea for dealing
with infinite depth, and that

0:12:08.400,0:12:10.540
is, do something depth-limited.

0:12:10.540,0:12:13.520
But we need to be a little bit careful,
because we don't necessarily

0:12:13.520,0:12:17.000
know how deep we want to
go until we've done it.

0:12:17.000,0:12:19.470
But the basic idea is we're going
to do some kind of depth-limited

0:12:19.470,0:12:24.310
computation, and we'll increase the
depth until the change is small, and

0:12:24.310,0:12:28.550
then we'll say, OK, we're close enough,
and we'll declare victory.

0:12:28.550,0:12:32.870
Now the reason why that might work is
because, if we have a discount, gamma,

0:12:32.870,0:12:35.840
then as you go deeper into the tree,
you're seeing those same rewards-- the

0:12:35.840,0:12:38.400
plus 1's, the plus 2's,
the minus 10's--

0:12:38.400,0:12:41.240
except as they get buried deeper and
deeper in the tree, they start picking

0:12:41.240,0:12:42.660
up factors of gamma.

0:12:42.660,0:12:45.690
And if you go 100 steps into the tree,
they've got a gamma to the 100 in

0:12:45.690,0:12:46.420
front of them.

0:12:46.420,0:12:49.230
And so if you go deep enough into the
tree, even though the tree goes on

0:12:49.230,0:12:52.230
forever, eventually it stops
contributing very much, because it's

0:12:52.230,0:12:55.520
out of the agent's horizon
in a soft way.

0:12:55.520,0:12:55.800
All right.

0:12:55.800,0:13:00.380
So how can we not do Expectimax in a way
that does all this work over and

0:13:00.380,0:13:04.210
over again, and then never terminates
because the tree is infinite?

0:13:04.210,0:13:07.250
The key idea here is we'll have
time-limited values, which you can

0:13:07.250,0:13:09.650
think of as a stopwatch.

0:13:09.650,0:13:13.420
Even though this game might in fact go
on forever, we've got a stopwatch

0:13:13.420,0:13:17.060
that's counting down the number of
timesteps remaining until, as far as

0:13:17.060,0:13:20.150
the agent's concerned,
the end of days.

0:13:20.150,0:13:25.100
And you've got k steps remaining,
and then the game just stops.

0:13:25.100,0:13:29.940
So we can define here not V*, which
would be some optimal thing letting

0:13:29.940,0:13:31.420
the game run forever.

0:13:31.420,0:13:34.560
We can define something much more
concrete and easy to think about,

0:13:34.560,0:13:39.410
which is V sub k, the time-limited
value of a state.

0:13:39.410,0:13:44.230
V sub k is the optimal value
of s if the game ends after

0:13:44.230,0:13:46.320
exactly k more timesteps.

0:13:46.320,0:13:47.460
What's a timestep?

0:13:47.460,0:13:48.520
It's a reward.

0:13:48.520,0:13:51.210
So exactly k more rewards.

0:13:51.210,0:13:54.650
This is actually kind of nice, because
this is exactly what a depth-k

0:13:54.650,0:13:58.330
Expectimax would give if
we started it from s.

0:13:58.330,0:14:02.090
So if you think about this tree here,
this is not the whole tree from the

0:14:02.090,0:14:03.070
blue states.

0:14:03.070,0:14:05.420
However, this is a depth-2 tree.

0:14:05.420,0:14:06.540
There's two rewards.

0:14:06.540,0:14:07.780
Where do the rewards come?

0:14:07.780,0:14:08.950
This is actually pretty important.

0:14:08.950,0:14:12.210
The rewards come here when this chance
layer resolves, and they come here

0:14:12.210,0:14:13.280
when this chance layer resolves.

0:14:13.280,0:14:14.960
That's where the transitions are.

0:14:14.960,0:14:17.330
OK, this is V sub 2.

0:14:17.330,0:14:20.420
If I ran this Expectimax and
I truncated it after two

0:14:20.420,0:14:23.550
steps, I get V sub 2.

0:14:23.550,0:14:27.090
And because we're just truncating and
we're just ending the game, we don't

0:14:27.090,0:14:29.850
need an evaluation function, because
we don't have to talk about what

0:14:29.850,0:14:31.130
happens after the game ends.

0:14:31.130,0:14:32.160
Nothing happens.

0:14:32.160,0:14:35.020
There are no rewards after two steps.

0:14:35.020,0:14:37.820
So let's take a look at what these
V sub k's actually are.

0:14:37.820,0:14:40.360
Then we'll figure out how to compute
them, and that's going to give us an

0:14:40.360,0:14:41.630
important algorithm.

0:14:41.630,0:14:41.970
All right.

0:14:41.970,0:14:44.230
So let's look here.

0:14:44.230,0:14:49.360
What this shows right now is, from
each of the states, what you can

0:14:49.360,0:14:54.530
achieve under optimal play if you are
allowed to take zero more actions.

0:14:54.530,0:14:57.510
Well, if there are zero more timesteps,
I'm not getting any more

0:14:57.510,0:15:01.370
rewards, which means the pit, the
gem, the wall, whatever--

0:15:01.370,0:15:03.040
it's all zero.

0:15:03.040,0:15:06.190
What happens if I can
take one more step?

0:15:06.190,0:15:09.630
Well, what happens if I'm
in the good exit?

0:15:09.630,0:15:13.640
Well, I have time to take
exit and get my plus 1.

0:15:13.640,0:15:17.280
If I'm in the bad exit, I have time
to take exit and get my minus 1.

0:15:17.280,0:15:20.730
And from anywhere else,
what can I achieve?

0:15:20.730,0:15:22.740
Well, it depends on the living
reward, but let's imagine the

0:15:22.740,0:15:24.250
living reward is zero.

0:15:24.250,0:15:26.360
I can achieve zero.

0:15:26.360,0:15:31.230
If I switch to V sub 1, then you can
see here that we have a plus 1, a

0:15:31.230,0:15:32.870
minus 1, and a whole bunch of zeros.

0:15:32.870,0:15:37.090
So you can see that from this
ledge, we get a zero.

0:15:37.090,0:15:39.050
Because in one timestep,
what can I do?

0:15:39.050,0:15:40.660
I can move north, I can
jump into the pit.

0:15:40.660,0:15:41.500
It doesn't matter.

0:15:41.500,0:15:43.830
I can't get any of the plus
1's or minus 1's.

0:15:43.830,0:15:47.230
Even here, where I have time to get to
the good exit, I don't have time to

0:15:47.230,0:15:48.990
leave and get my reward.

0:15:48.990,0:15:55.370
If I'm allowed two timesteps, you can
see that from this square, I have time

0:15:55.370,0:15:59.140
with some probability to get into
the good exit and leave

0:15:59.140,0:16:02.680
and receive my reward.

0:16:02.680,0:16:08.160
From here, the ledge, I have time to do
various stupid things, like go into

0:16:08.160,0:16:09.730
the pit and receive a negative.

0:16:09.730,0:16:10.940
But that's not the optimal thing.

0:16:10.940,0:16:13.340
The optimal thing is anything else.

0:16:13.340,0:16:14.670
And so I have a zero.

0:16:14.670,0:16:17.620
But if I'm allowed three steps, I can
get some rewards even from there.

0:16:17.620,0:16:19.570
And four steps, and five steps.

0:16:19.570,0:16:24.590
How many steps would it take for this
start state, if the lower left is the

0:16:24.590,0:16:30.160
start state, to have a value V sub
k which is greater than zero?

0:16:30.160,0:16:36.310
Well the shortest way I could possibly
get to a positive reward is one, two,

0:16:36.310,0:16:39.000
three, four, five, and then
six to take the exit.

0:16:39.000,0:16:40.360
So I look at V sub 6.

0:16:40.360,0:16:43.790
I can see that I have now 0.34 here.

0:16:43.790,0:16:45.970
Now what happens if I
allow seven steps?

0:16:45.970,0:16:50.260
Is this 0.34 going to go up or
is it going to stay the same?

0:16:50.260,0:16:54.100
Well, if I'm allowed seven steps, I can
not only get there in the lucky

0:16:54.100,0:16:55.410
way where nothing goes wrong.

0:16:55.410,0:16:59.060
I can also get there in various ways
where something goes wrong once.

0:16:59.060,0:17:02.160
So in fact, if I go to depth
7, that will increase.

0:17:02.160,0:17:05.890
And if I go all the way to depth
100, then I start to see

0:17:05.890,0:17:07.220
the particular values.

0:17:07.220,0:17:11.220
And now at depth 100, because of the
specific settings of this problem,

0:17:11.220,0:17:12.790
most of the states are pretty good.

0:17:12.790,0:17:14.710
It's pretty much, as long as you're
not in the pit, you get

0:17:14.710,0:17:15.849
a reasonable value.

0:17:15.849,0:17:18.829
And of course, these actual numbers and
how quickly they get smaller as

0:17:18.829,0:17:22.050
you go away from the reward will depend
on the specific probabilities

0:17:22.050,0:17:25.859
involved and discounts and so on,
in the instance of grid world.

0:17:25.859,0:17:26.700
All right.

0:17:26.700,0:17:29.720
So we saw what these V sub k's are.

0:17:29.720,0:17:35.340
They are your Expectimax value under
optimal play if the game

0:17:35.340,0:17:37.140
ends after k steps.

0:17:37.140,0:17:40.020
Now we can go back and we can look at
this big Expectimax tree, and we can

0:17:40.020,0:17:43.580
think about how V sub k
relates to this tree.

0:17:43.580,0:17:46.470
Let's imagine that in fact this
is the whole tree and it

0:17:46.470,0:17:47.560
doesn't go down to infinity.

0:17:47.560,0:17:50.020
It's hard to fit infinite trees
onto your PowerPoint.

0:17:50.020,0:17:52.990
So if this is the whole tree and we
looked at the bottom, we would say,

0:17:52.990,0:17:54.630
all right, what is at the bottom?

0:17:54.630,0:17:58.280
Well, it's a huge number of states I
would have to check in Expectimax.

0:17:58.280,0:18:01.060
But there's kind of only three
things down there, right?

0:18:01.060,0:18:03.740
Because there's only three
states in this MDP.

0:18:03.740,0:18:06.220
And for each one of these,
it's a terminal state.

0:18:06.220,0:18:09.260
Nothing else happens,
so no more rewards.

0:18:09.260,0:18:12.440
And that means that I've got,
essentially, lots and lots of copies

0:18:12.440,0:18:15.280
of V0, lots of depth-zero Expectimax.

0:18:15.280,0:18:17.240
In fact, we know what V0 is.

0:18:17.240,0:18:19.700
It's zero for any state.

0:18:19.700,0:18:21.800
Now what's in this layer up here?

0:18:21.800,0:18:24.170
Well, if I look, again we've
got a bunch of repetitions.

0:18:24.170,0:18:29.200
But each one of these computations is
a depth-one Expectimax computation.

0:18:29.200,0:18:31.920
Which means, in fact, all we have
is a whole bunch of copies

0:18:31.920,0:18:34.530
of V1 in this layer.

0:18:34.530,0:18:37.450
And if we go up, we've got a whole
bunch of copies of V2.

0:18:37.450,0:18:40.010
And then a whole bunch
of copies of V3.

0:18:40.010,0:18:42.510
Except now we're actually close enough
to the top that it's not that many.

0:18:42.510,0:18:45.560
And then at the very top, we're
only computing one value.

0:18:45.560,0:18:49.130
But conceptually this top layer
is well-represented by the

0:18:49.130,0:18:50.380
collection of V4's.

0:18:50.380,0:18:52.580

0:18:52.580,0:18:56.750
So do you see how, at the bottom, even
though the tree has grown immensely,

0:18:56.750,0:18:58.790
it's still only three values?

0:18:58.790,0:19:02.990
And each layer is V sub
k for a larger k.

0:19:02.990,0:19:07.160
So this actually gives us an idea of how
we can compute these values in an

0:19:07.160,0:19:11.710
efficient way, where we don't get the
explosion in depth, because you can

0:19:11.710,0:19:13.790
see at the bottom it's no
worse than at the top.

0:19:13.790,0:19:15.040

