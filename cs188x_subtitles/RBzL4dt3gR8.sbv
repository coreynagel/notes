0:00:00.000,0:00:01.680

0:00:01.680,0:00:03.700
PROFESSOR: This brings us to one
of t he most important ideas in

0:00:03.700,0:00:07.440
reinforcement learning, and that is
because you want to do the optimal

0:00:07.440,0:00:11.690
thing but you don't know what it is
yet, you have this inevitable

0:00:11.690,0:00:14.930
trade-off between exploration,
where you try things--

0:00:14.930,0:00:17.050
which may, of course, be disastrous--

0:00:17.050,0:00:19.710
and exploitation, where you do
the things which currently

0:00:19.710,0:00:21.330
appear to be good.

0:00:21.330,0:00:23.590
You meet this all the time
in your daily life.

0:00:23.590,0:00:25.700
Some new restaurant opens
and you have a choice.

0:00:25.700,0:00:28.170
You can go to your favorite place, which
you know will bring you good

0:00:28.170,0:00:31.730
rewards, or you can try this new
thing, which is probably bad.

0:00:31.730,0:00:34.010
But it may be your new
favorite restaurant.

0:00:34.010,0:00:36.390
If you try it, you may
have a bad meal.

0:00:36.390,0:00:37.900
You may get food poisoning.

0:00:37.900,0:00:40.580
But if you don't try it, you may
be missing out on your favorite

0:00:40.580,0:00:42.930
restaurant until the end of time.

0:00:42.930,0:00:46.880
And so inevitably, the outcome is
you have to try some stuff.

0:00:46.880,0:00:49.120
And that means you have
to make some mistakes.

0:00:49.120,0:00:51.540
Let's take a look at that.

0:00:51.540,0:00:53.600
So this one's manual.

0:00:53.600,0:00:56.560
This is a variance of the
grid world, where--

0:00:56.560,0:00:58.300
well, I won't tell you what it is.

0:00:58.300,0:01:00.730
You look at this and you
think how should I act?

0:01:00.730,0:01:02.110
What is this?

0:01:02.110,0:01:04.904
Well, you've been listening to this
lecture long enough that you know this

0:01:04.904,0:01:08.000
is probably some fire pits on the
side of a bridge or something.

0:01:08.000,0:01:12.500
But for all you know, this is just 12
doors, one of which has a huge prize.

0:01:12.500,0:01:15.640
So first of all, just looking at this,
you kind of have to try everything.

0:01:15.640,0:01:17.150
Now, what if we did this?

0:01:17.150,0:01:19.990
What if we went over there and
we did that a couple times?

0:01:19.990,0:01:23.940
It looks like we're going to
get plus 1 if we go there.

0:01:23.940,0:01:27.480
Now if all we did was exploit, we would
continue going left forever and

0:01:27.480,0:01:30.870
we would receive a string of plus
1's, episode after episode.

0:01:30.870,0:01:32.110
Is that the best we can do?

0:01:32.110,0:01:33.530
Well, maybe.

0:01:33.530,0:01:35.580
We could be following the optimal
policy right now.

0:01:35.580,0:01:38.890
But, in some sense, we've
got to try other stuff.

0:01:38.890,0:01:40.860
Because we're going to be playing
this game for a while.

0:01:40.860,0:01:42.690
As far as we know, we're going
to play it forever.

0:01:42.690,0:01:43.770
And so we need to explore.

0:01:43.770,0:01:47.850
So we might do things like
jump off the cliff.

0:01:47.850,0:01:48.920
That was bad.

0:01:48.920,0:01:49.450
OK.

0:01:49.450,0:01:51.210
Maybe we'll jump off again,
just to make sure we

0:01:51.210,0:01:52.360
didn't get a weird outcome.

0:01:52.360,0:01:54.040
No, it's looking pretty bad.

0:01:54.040,0:01:56.580
We might go over here.

0:01:56.580,0:01:56.666
Oh.

0:01:56.666,0:01:57.140
That one's good.

0:01:57.140,0:01:58.970
It's so good that now the other
one doesn't even look

0:01:58.970,0:02:01.030
good to the left anymore.

0:02:01.030,0:02:03.620
And so now, we go here.

0:02:03.620,0:02:04.240
Now are done?

0:02:04.240,0:02:06.940
Well, we've got a lot of
things we should try.

0:02:06.940,0:02:09.669
Now, let me point out something, which
is that you have a lot of knowledge

0:02:09.669,0:02:13.490
about the structure of this MDP, so once
you see one fire pit, you imagine

0:02:13.490,0:02:14.560
maybe there are some others.

0:02:14.560,0:02:17.390
The algorithm has no such
concept right now.

0:02:17.390,0:02:20.730
As far as we know, jumping off this
cliff will be equally good as the good

0:02:20.730,0:02:22.790
exit, but in fact, it's
equally unsuccessful.

0:02:22.790,0:02:23.330
Well, what about this pit?

0:02:23.330,0:02:24.820
We've got to try that too.

0:02:24.820,0:02:27.650
So you spend a lot of time jumping
into pits because you really just

0:02:27.650,0:02:29.750
don't know their pits
until you try it.

0:02:29.750,0:02:30.980
So you've got to make some mistakes.

0:02:30.980,0:02:32.270
You've got to explore.

0:02:32.270,0:02:35.360
But, of course, you don't want to just
be jumping in them over and over again

0:02:35.360,0:02:36.500
once you know they're bad.

0:02:36.500,0:02:39.920
So you don't want to explore
randomly or forever.

0:02:39.920,0:02:41.390
We know we've got to try things.

0:02:41.390,0:02:44.110
The simplest way to force exploration
is what's called an

0:02:44.110,0:02:45.850
epsilon greedy policy.

0:02:45.850,0:02:50.270
So let's remember that Q-Learning did
not specify how we selected actions,

0:02:50.270,0:02:53.260
it just required that they have
sufficient variety to converge to the

0:02:53.260,0:02:55.530
optimal policy and Q values.

0:02:55.530,0:02:58.700
Now we're talking about how to select
actions in a way that will enable

0:02:58.700,0:03:00.780
Q-Learning to do it's magic.

0:03:00.780,0:03:04.140
The simplest thing we could do is be
epsilon greedy, which means at every

0:03:04.140,0:03:08.880
time step, with some presumably small
probability, we just act randomly,

0:03:08.880,0:03:10.700
just to try something out.

0:03:10.700,0:03:13.960
And with, presumably, a large
probability, we act according to our

0:03:13.960,0:03:17.480
current policies best guess, which
means we often go to our favorite

0:03:17.480,0:03:21.300
place, but every now and then when
the coin comes up random, we try

0:03:21.300,0:03:23.230
something at random.

0:03:23.230,0:03:25.840
There are some problems with
taking random actions.

0:03:25.840,0:03:28.150
Let's see an agent taking random
action and see what

0:03:28.150,0:03:29.790
those problems are.

0:03:29.790,0:03:32.050
So here's the crawler.

0:03:32.050,0:03:35.280
And now you can finally see what
this epsilon control here is.

0:03:35.280,0:03:37.980
This means 80% of the time,
we take a random action.

0:03:37.980,0:03:40.270
That's a lot of exploration.

0:03:40.270,0:03:41.926
It's not, however, a lot of progress.

0:03:41.926,0:03:46.080

0:03:46.080,0:03:46.765
Now will this work?

0:03:46.765,0:03:49.910
K Q-Learning works just fine with
lots of random actions.

0:03:49.910,0:03:51.310
It just doesn't necessarily
work quickly.

0:03:51.310,0:03:52.790
So I skip some steps.

0:03:52.790,0:03:56.100
And we've got some vague clue.

0:03:56.100,0:03:58.830
I'm going to skip a million steps.

0:03:58.830,0:04:01.340
So we compute and we compute and
we compute and we compute.

0:04:01.340,0:04:05.645
And now it actually turns out that this
bot has had a lot of experience

0:04:05.645,0:04:07.130
this year, even though
I fast forwarded it.

0:04:07.130,0:04:08.860
Presumably it should have
learned something.

0:04:08.860,0:04:11.390
Why is it not making any progress?

0:04:11.390,0:04:15.450
Well, we're forcing it to act
randomly 80% of the time.

0:04:15.450,0:04:18.450
If whenever you walked, your left leg,
80% of the time, flew out at some

0:04:18.450,0:04:20.079
random angle, you wouldn't
make fast progress.

0:04:20.079,0:04:22.660

0:04:22.660,0:04:25.470
So what happens if I
turn epsilon down.

0:04:25.470,0:04:27.440
Turn it off, and just let it go.

0:04:27.440,0:04:29.240
Be optimal.

0:04:29.240,0:04:30.500
It's not so bad.

0:04:30.500,0:04:33.890
So Q-Learning learned the right thing,
but the exploration was preventing you

0:04:33.890,0:04:34.790
from doing it.

0:04:34.790,0:04:36.670
So there were a couple issues there.

0:04:36.670,0:04:38.410
Let's see what they were.

0:04:38.410,0:04:42.560
One issue is you do eventually explore
the space, you eventually learn the

0:04:42.560,0:04:45.360
right thing, but you still keep
thrashing around if you

0:04:45.360,0:04:46.820
keep epsilon high.

0:04:46.820,0:04:50.440
So at a bare minimum, you would
eventually have to stop exploring so

0:04:50.440,0:04:51.600
much random.

0:04:51.600,0:04:55.800
Eventually, you would have to stop doing
so much exploration and allow

0:04:55.800,0:04:58.880
yourself to exploit this optimal
policy you've learned.

0:04:58.880,0:05:04.630
Another problem with epsilon greedy is
that the exploration is unstructured.

0:05:04.630,0:05:08.340
You try random things whether you need
to or not, even if you know what the

0:05:08.340,0:05:08.760
actions do.

0:05:08.760,0:05:11.100
If you know what all the actions
do, you shouldn't necessarily

0:05:11.100,0:05:12.670
be exploring anymore.

0:05:12.670,0:05:15.630
So one solution here is we just lower
epsilon over time and let the

0:05:15.630,0:05:16.760
randomness decrease.

0:05:16.760,0:05:18.720
But there are better ways, and there
are a lot of better ways.

0:05:18.720,0:05:20.720
I'm going to sketch one simple
one, which is what's called

0:05:20.720,0:05:22.730
an exploration function.

0:05:22.730,0:05:24.420
So why should we explore?

0:05:24.420,0:05:28.520
Well, random actions explore a fixed
amount, and they kind of explore

0:05:28.520,0:05:29.730
uniformly everywhere.

0:05:29.730,0:05:33.710
Every state has an equal chance of
doing an exploration action.

0:05:33.710,0:05:34.970
Here's a better idea.

0:05:34.970,0:05:38.730
Why don't we think about our actions,
and thereby our Q values, as being

0:05:38.730,0:05:40.270
something we're trying to learn?

0:05:40.270,0:05:43.120
Now, sometimes we know what they are,
and when we know what they are, we

0:05:43.120,0:05:44.310
don't have to try them anymore.

0:05:44.310,0:05:46.700
You get food poisoning a couple
times and you just stop.

0:05:46.700,0:05:50.260
On the other, when we don't know
something and our uncertainty is high,

0:05:50.260,0:05:53.600
it should be a pretty high priority
to figure that out right now.

0:05:53.600,0:05:56.760
So when the restaurant opens, you try
it, because your uncertainty is high.

0:05:56.760,0:05:58.802
Once you've tried it a couple
times, now you know.

0:05:58.802,0:06:00.330
So how can we encode this?

0:06:00.330,0:06:04.060
We'd like something that forces us to
explore areas whose badness is not yet

0:06:04.060,0:06:06.010
established, but eventually stopped.

0:06:06.010,0:06:09.520
And the basic idea here is is the quote
you often hear is, in the face

0:06:09.520,0:06:11.950
of uncertainty, you should
have optimism.

0:06:11.950,0:06:15.870
So fly into those unknown caves
and see what's in there.

0:06:15.870,0:06:19.050
Optimism shouldn't last forever, so we
might have a function like this.

0:06:19.050,0:06:20.820
Here's a very crude way of doing it.

0:06:20.820,0:06:25.470
But this basically accomplishes the goal
I sketched, which is rather than

0:06:25.470,0:06:29.740
looking at just utilities of Q states
or states, we have a function which

0:06:29.740,0:06:33.750
considers our guess at the utility,
u, and the number of

0:06:33.750,0:06:35.580
times we've been there.

0:06:35.580,0:06:38.050
And from a Q state, the number of
times we've been there means the

0:06:38.050,0:06:40.890
number of times we've tried
that action out.

0:06:40.890,0:06:46.240
So we take the utility and we add to it
a bonus that decrease as we visit

0:06:46.240,0:06:48.870
the state more times, n.

0:06:48.870,0:06:51.120
So the regular Q update
looks like this.

0:06:51.120,0:06:55.100
It says take your current estimate
of Qsa for the action, a, you

0:06:55.100,0:06:56.500
just tried from s.

0:06:56.500,0:07:00.730
This arrow means blend in this new value
on the right- hand side with

0:07:00.730,0:07:01.790
weight alpha.

0:07:01.790,0:07:06.130
So this says take our estimate and
replace it with a one-step look ahead.

0:07:06.130,0:07:10.110
And it's a sample of the instantaneous
reward we got plus our current best

0:07:10.110,0:07:13.840
estimate of the future value in
the landing state, s prime.

0:07:13.840,0:07:16.960
With an exploration function,
we can modify this.

0:07:16.960,0:07:22.950
We can modify this by replacing the Q
value with a boosted Q value, which is

0:07:22.950,0:07:27.680
artificially inflated if we haven't
tried that Q state many times, meaning

0:07:27.680,0:07:29.460
we're uncertain.

0:07:29.460,0:07:30.990
And what this does is kind of cool.

0:07:30.990,0:07:34.470
Not only does this tell you that a Q
state you haven't tried has higher

0:07:34.470,0:07:38.130
value, but also it will propagate
this bonus back.

0:07:38.130,0:07:41.840
So you don't just try things that are
unknown, you try things that are known

0:07:41.840,0:07:44.215
to lead to states that are unknown.

0:07:44.215,0:07:45.310
And that's great.

0:07:45.310,0:07:47.320
Let's see it in action.

0:07:47.320,0:07:51.930
So here is that same bot.

0:07:51.930,0:07:56.520
It's still doing a tiny bit of
exploration through randomness, but

0:07:56.520,0:07:58.900
it's implementing an exploration
function.

0:07:58.900,0:08:01.750

0:08:01.750,0:08:04.240
So this bot is implementing
an exploration function.

0:08:04.240,0:08:06.650
And as time goes on, the exploration
function will

0:08:06.650,0:08:07.980
contribute less and less.

0:08:07.980,0:08:10.010
And what that means is that even though
at the beginning it's trying

0:08:10.010,0:08:14.160
all kinds of stuff, very quickly it
figures out that it actually knows

0:08:14.160,0:08:15.940
what those actions do.

0:08:15.940,0:08:18.790
And the dominant behavior is
now one of exploitation.

0:08:18.790,0:08:22.630

0:08:22.630,0:08:27.000
And it's a slightly weird
policy, but he's off.

0:08:27.000,0:08:30.050
So unlike the other one, which had to
run millions of steps and then I had

0:08:30.050,0:08:33.990
to turn its exploration off, this one
is already moving after a very small

0:08:33.990,0:08:36.360
number of iterations.

0:08:36.360,0:08:39.960
Regret is another one of those terms
that you use all the time in daily

0:08:39.960,0:08:43.549
life, but it means something specific
here in this course.

0:08:43.549,0:08:47.440
The basic idea of regret is even though
you learn the optimal policy

0:08:47.440,0:08:51.940
eventually, because your transitions
and rewards are initially unknown,

0:08:51.940,0:08:54.900
it's inevitable that you will make
some mistakes along the way.

0:08:54.900,0:08:58.240
So you don't get to be the wise, optimal
robot without making some

0:08:58.240,0:08:59.870
mistakes in your youth.

0:08:59.870,0:09:01.400
So, for example, here, this robot.

0:09:01.400,0:09:04.480
It's wise, it's optimal, and it's
remembering back to its youth where it

0:09:04.480,0:09:05.950
jumped into that fire pit.

0:09:05.950,0:09:08.640
Now, it knows now not to do that.

0:09:08.640,0:09:10.890
And you can think of this as regret.

0:09:10.890,0:09:14.210
But a certain amount of fire pit jumping
is inevitable, because you

0:09:14.210,0:09:16.790
just had no way of knowing whether
or not it was a fire pit

0:09:16.790,0:09:18.540
until you tried it.

0:09:18.540,0:09:21.340
So what is regret formally
in this class?

0:09:21.340,0:09:25.200
Formally, regret is a measure
of your total mistakes.

0:09:25.200,0:09:30.600
It's the difference between you rewards
that you got, fire pit testing

0:09:30.600,0:09:36.050
and all, compared to the optimal rewards
you would have gotten had you

0:09:36.050,0:09:39.010
had the optimal policy all along.

0:09:39.010,0:09:41.860
Now, of course, all those rewards have
to be expected rewards, because you've

0:09:41.860,0:09:44.880
got to average all the stuff over
all the possible outcomes.

0:09:44.880,0:09:48.350
But there's this idea that there's a
total volume of mistakes you made

0:09:48.350,0:09:51.510
compared to having the optimal policy.

0:09:51.510,0:09:53.090
We'd like to minimize that.

0:09:53.090,0:09:56.460
So we've already seen Q-Learning subject
to some mild conditions.

0:09:56.460,0:09:58.690
We'll eventually learn
the optimal policy.

0:09:58.690,0:10:00.350
You're going to be optimal in the end.

0:10:00.350,0:10:03.360
So really, in practice, the game
is minimizing your regret.

0:10:03.360,0:10:07.240
Get to that optimal policy while
making as few suboptimal, in

0:10:07.240,0:10:10.440
retrospect, actions as possible.

0:10:10.440,0:10:12.800
Minimizing regret is more than
learning to be optimal.

0:10:12.800,0:10:15.960
It's more like optimally
learning to be optimal.

0:10:15.960,0:10:19.480
And one example of this is random
exploration, like epsilon greedy,

0:10:19.480,0:10:21.410
compared to an exploration function.

0:10:21.410,0:10:24.090
You'll be optimal in the end with
Q-Learning either way, but you'll have

0:10:24.090,0:10:27.320
higher regret with random exploration,
because with random explanation, you

0:10:27.320,0:10:30.250
kind of keep making mistakes and
your mistakes are unguided.

0:10:30.250,0:10:33.710
You make them even when you know
yo u shouldn't do them.

0:10:33.710,0:10:34.960
So that's regret.

0:10:34.960,0:10:35.330

